{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6faee587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "330/330 [==============================] - 0s 424us/step - loss: 2.4526 - accuracy: 0.1567 - mse: 0.3109\n",
      "Epoch 2/150\n",
      "330/330 [==============================] - 0s 421us/step - loss: 2.3125 - accuracy: 0.1799 - mse: 0.3349\n",
      "Epoch 3/150\n",
      "330/330 [==============================] - 0s 424us/step - loss: 2.2810 - accuracy: 0.1865 - mse: 0.3400\n",
      "Epoch 4/150\n",
      "330/330 [==============================] - 0s 421us/step - loss: 2.2636 - accuracy: 0.1892 - mse: 0.3433\n",
      "Epoch 5/150\n",
      "330/330 [==============================] - 0s 418us/step - loss: 2.2501 - accuracy: 0.1965 - mse: 0.3485\n",
      "Epoch 6/150\n",
      "330/330 [==============================] - 0s 424us/step - loss: 2.2369 - accuracy: 0.1994 - mse: 0.3505\n",
      "Epoch 7/150\n",
      "330/330 [==============================] - 0s 433us/step - loss: 2.2290 - accuracy: 0.2038 - mse: 0.3511\n",
      "Epoch 8/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 2.2182 - accuracy: 0.2059 - mse: 0.3539\n",
      "Epoch 9/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 2.2087 - accuracy: 0.2088 - mse: 0.3539\n",
      "Epoch 10/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 2.2084 - accuracy: 0.2048 - mse: 0.3562\n",
      "Epoch 11/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 2.1991 - accuracy: 0.2084 - mse: 0.3564\n",
      "Epoch 12/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.1914 - accuracy: 0.2128 - mse: 0.3596\n",
      "Epoch 13/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.1858 - accuracy: 0.2148 - mse: 0.3590\n",
      "Epoch 14/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.1764 - accuracy: 0.2167 - mse: 0.3617\n",
      "Epoch 15/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.1719 - accuracy: 0.2179 - mse: 0.3614\n",
      "Epoch 16/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 2.1666 - accuracy: 0.2153 - mse: 0.3611\n",
      "Epoch 17/150\n",
      "330/330 [==============================] - 0s 427us/step - loss: 2.1631 - accuracy: 0.2166 - mse: 0.3614\n",
      "Epoch 18/150\n",
      "330/330 [==============================] - 0s 452us/step - loss: 2.1603 - accuracy: 0.2172 - mse: 0.3618\n",
      "Epoch 19/150\n",
      "330/330 [==============================] - 0s 433us/step - loss: 2.1511 - accuracy: 0.2173 - mse: 0.3639\n",
      "Epoch 20/150\n",
      "330/330 [==============================] - 0s 427us/step - loss: 2.1474 - accuracy: 0.2185 - mse: 0.3639\n",
      "Epoch 21/150\n",
      "330/330 [==============================] - 0s 443us/step - loss: 2.1424 - accuracy: 0.2223 - mse: 0.3659\n",
      "Epoch 22/150\n",
      "330/330 [==============================] - 0s 433us/step - loss: 2.1385 - accuracy: 0.2244 - mse: 0.3667\n",
      "Epoch 23/150\n",
      "330/330 [==============================] - 0s 424us/step - loss: 2.1365 - accuracy: 0.2270 - mse: 0.3645\n",
      "Epoch 24/150\n",
      "330/330 [==============================] - 0s 421us/step - loss: 2.1351 - accuracy: 0.2289 - mse: 0.3656\n",
      "Epoch 25/150\n",
      "330/330 [==============================] - 0s 427us/step - loss: 2.1260 - accuracy: 0.2294 - mse: 0.3655\n",
      "Epoch 26/150\n",
      "330/330 [==============================] - 0s 414us/step - loss: 2.1225 - accuracy: 0.2329 - mse: 0.3678\n",
      "Epoch 27/150\n",
      "330/330 [==============================] - 0s 440us/step - loss: 2.1180 - accuracy: 0.2302 - mse: 0.3678\n",
      "Epoch 28/150\n",
      "330/330 [==============================] - 0s 482us/step - loss: 2.1148 - accuracy: 0.2308 - mse: 0.3672\n",
      "Epoch 29/150\n",
      "330/330 [==============================] - 0s 437us/step - loss: 2.1113 - accuracy: 0.2304 - mse: 0.3674\n",
      "Epoch 30/150\n",
      "330/330 [==============================] - 0s 458us/step - loss: 2.1061 - accuracy: 0.2324 - mse: 0.3678\n",
      "Epoch 31/150\n",
      "330/330 [==============================] - 0s 430us/step - loss: 2.1050 - accuracy: 0.2320 - mse: 0.3678\n",
      "Epoch 32/150\n",
      "330/330 [==============================] - 0s 430us/step - loss: 2.0973 - accuracy: 0.2329 - mse: 0.3676\n",
      "Epoch 33/150\n",
      "330/330 [==============================] - 0s 424us/step - loss: 2.0945 - accuracy: 0.2376 - mse: 0.3678\n",
      "Epoch 34/150\n",
      "330/330 [==============================] - 0s 427us/step - loss: 2.0895 - accuracy: 0.2429 - mse: 0.3681\n",
      "Epoch 35/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0873 - accuracy: 0.2384 - mse: 0.3705\n",
      "Epoch 36/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0823 - accuracy: 0.2380 - mse: 0.3696\n",
      "Epoch 37/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0752 - accuracy: 0.2479 - mse: 0.3700\n",
      "Epoch 38/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 2.0760 - accuracy: 0.2394 - mse: 0.3704\n",
      "Epoch 39/150\n",
      "330/330 [==============================] - 0s 403us/step - loss: 2.0688 - accuracy: 0.2460 - mse: 0.3705\n",
      "Epoch 40/150\n",
      "330/330 [==============================] - 0s 421us/step - loss: 2.0694 - accuracy: 0.2467 - mse: 0.3699\n",
      "Epoch 41/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 2.0639 - accuracy: 0.2478 - mse: 0.3693\n",
      "Epoch 42/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0623 - accuracy: 0.2526 - mse: 0.3697\n",
      "Epoch 43/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0556 - accuracy: 0.2519 - mse: 0.3717\n",
      "Epoch 44/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0520 - accuracy: 0.2491 - mse: 0.3724\n",
      "Epoch 45/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 2.0475 - accuracy: 0.2524 - mse: 0.3716\n",
      "Epoch 46/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0504 - accuracy: 0.2508 - mse: 0.3713\n",
      "Epoch 47/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 2.0416 - accuracy: 0.2600 - mse: 0.3711\n",
      "Epoch 48/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0362 - accuracy: 0.2622 - mse: 0.3734\n",
      "Epoch 49/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0388 - accuracy: 0.2590 - mse: 0.3733\n",
      "Epoch 50/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0312 - accuracy: 0.2608 - mse: 0.3748\n",
      "Epoch 51/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0311 - accuracy: 0.2554 - mse: 0.3744\n",
      "Epoch 52/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 2.0274 - accuracy: 0.2501 - mse: 0.3747\n",
      "Epoch 53/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0259 - accuracy: 0.2594 - mse: 0.3728\n",
      "Epoch 54/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0225 - accuracy: 0.2622 - mse: 0.3760\n",
      "Epoch 55/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0174 - accuracy: 0.2683 - mse: 0.3753\n",
      "Epoch 56/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0110 - accuracy: 0.2677 - mse: 0.3767\n",
      "Epoch 57/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 2.0115 - accuracy: 0.2638 - mse: 0.3765\n",
      "Epoch 58/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0076 - accuracy: 0.2710 - mse: 0.3770\n",
      "Epoch 59/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0062 - accuracy: 0.2671 - mse: 0.3773\n",
      "Epoch 60/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 2.0007 - accuracy: 0.2743 - mse: 0.3774\n",
      "Epoch 61/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9938 - accuracy: 0.2696 - mse: 0.3769\n",
      "Epoch 62/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9998 - accuracy: 0.2708 - mse: 0.3790\n",
      "Epoch 63/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9916 - accuracy: 0.2692 - mse: 0.3787\n",
      "Epoch 64/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9920 - accuracy: 0.2745 - mse: 0.3778\n",
      "Epoch 65/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9909 - accuracy: 0.2731 - mse: 0.3789\n",
      "Epoch 66/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9895 - accuracy: 0.2762 - mse: 0.3779\n",
      "Epoch 67/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9798 - accuracy: 0.2801 - mse: 0.3773\n",
      "Epoch 68/150\n",
      "330/330 [==============================] - 0s 421us/step - loss: 1.9796 - accuracy: 0.2779 - mse: 0.3789\n",
      "Epoch 69/150\n",
      "330/330 [==============================] - 0s 430us/step - loss: 1.9760 - accuracy: 0.2783 - mse: 0.3782\n",
      "Epoch 70/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 415us/step - loss: 1.9747 - accuracy: 0.2821 - mse: 0.3798\n",
      "Epoch 71/150\n",
      "330/330 [==============================] - 0s 433us/step - loss: 1.9723 - accuracy: 0.2798 - mse: 0.3795\n",
      "Epoch 72/150\n",
      "330/330 [==============================] - 0s 455us/step - loss: 1.9718 - accuracy: 0.2803 - mse: 0.3808\n",
      "Epoch 73/150\n",
      "330/330 [==============================] - 0s 427us/step - loss: 1.9707 - accuracy: 0.2799 - mse: 0.3790\n",
      "Epoch 74/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9596 - accuracy: 0.2810 - mse: 0.3795\n",
      "Epoch 75/150\n",
      "330/330 [==============================] - 0s 433us/step - loss: 1.9664 - accuracy: 0.2790 - mse: 0.3818\n",
      "Epoch 76/150\n",
      "330/330 [==============================] - 0s 427us/step - loss: 1.9622 - accuracy: 0.2809 - mse: 0.3816\n",
      "Epoch 77/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9549 - accuracy: 0.2841 - mse: 0.3821\n",
      "Epoch 78/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9582 - accuracy: 0.2779 - mse: 0.3825\n",
      "Epoch 79/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9544 - accuracy: 0.2820 - mse: 0.3820\n",
      "Epoch 80/150\n",
      "330/330 [==============================] - 0s 440us/step - loss: 1.9509 - accuracy: 0.2828 - mse: 0.3813\n",
      "Epoch 81/150\n",
      "330/330 [==============================] - 0s 440us/step - loss: 1.9509 - accuracy: 0.2827 - mse: 0.3802\n",
      "Epoch 82/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9500 - accuracy: 0.2802 - mse: 0.3802\n",
      "Epoch 83/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9426 - accuracy: 0.2818 - mse: 0.3833\n",
      "Epoch 84/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9445 - accuracy: 0.2855 - mse: 0.3850\n",
      "Epoch 85/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9399 - accuracy: 0.2851 - mse: 0.3840\n",
      "Epoch 86/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9353 - accuracy: 0.2916 - mse: 0.3840\n",
      "Epoch 87/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 1.9377 - accuracy: 0.2856 - mse: 0.3855\n",
      "Epoch 88/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9366 - accuracy: 0.2869 - mse: 0.3851\n",
      "Epoch 89/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9358 - accuracy: 0.2880 - mse: 0.3877\n",
      "Epoch 90/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9324 - accuracy: 0.2916 - mse: 0.3855\n",
      "Epoch 91/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9305 - accuracy: 0.2869 - mse: 0.3877\n",
      "Epoch 92/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9305 - accuracy: 0.2892 - mse: 0.3859\n",
      "Epoch 93/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9255 - accuracy: 0.2933 - mse: 0.3867\n",
      "Epoch 94/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9290 - accuracy: 0.2905 - mse: 0.3867\n",
      "Epoch 95/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9200 - accuracy: 0.2923 - mse: 0.3890\n",
      "Epoch 96/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9255 - accuracy: 0.2945 - mse: 0.3894\n",
      "Epoch 97/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9226 - accuracy: 0.2967 - mse: 0.3886\n",
      "Epoch 98/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9161 - accuracy: 0.2928 - mse: 0.3900\n",
      "Epoch 99/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9226 - accuracy: 0.2955 - mse: 0.3905\n",
      "Epoch 100/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9163 - accuracy: 0.2934 - mse: 0.3913\n",
      "Epoch 101/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9133 - accuracy: 0.2989 - mse: 0.3915\n",
      "Epoch 102/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9128 - accuracy: 0.2939 - mse: 0.3898\n",
      "Epoch 103/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9144 - accuracy: 0.2984 - mse: 0.3919\n",
      "Epoch 104/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 1.9167 - accuracy: 0.2971 - mse: 0.3919\n",
      "Epoch 105/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9125 - accuracy: 0.2970 - mse: 0.3920\n",
      "Epoch 106/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9116 - accuracy: 0.2944 - mse: 0.3931\n",
      "Epoch 107/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9146 - accuracy: 0.2971 - mse: 0.3922\n",
      "Epoch 108/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9074 - accuracy: 0.2959 - mse: 0.3911\n",
      "Epoch 109/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 1.9047 - accuracy: 0.2969 - mse: 0.3920\n",
      "Epoch 110/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.9092 - accuracy: 0.3005 - mse: 0.3929\n",
      "Epoch 111/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9023 - accuracy: 0.2975 - mse: 0.3930\n",
      "Epoch 112/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.9025 - accuracy: 0.3016 - mse: 0.3949\n",
      "Epoch 113/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8975 - accuracy: 0.3000 - mse: 0.3965\n",
      "Epoch 114/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8961 - accuracy: 0.3013 - mse: 0.3967\n",
      "Epoch 115/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8990 - accuracy: 0.3024 - mse: 0.3972\n",
      "Epoch 116/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8940 - accuracy: 0.2984 - mse: 0.3960\n",
      "Epoch 117/150\n",
      "330/330 [==============================] - 0s 443us/step - loss: 1.8932 - accuracy: 0.3020 - mse: 0.3975\n",
      "Epoch 118/150\n",
      "330/330 [==============================] - 0s 421us/step - loss: 1.8931 - accuracy: 0.3083 - mse: 0.3974\n",
      "Epoch 119/150\n",
      "330/330 [==============================] - 0s 461us/step - loss: 1.8899 - accuracy: 0.3044 - mse: 0.3977\n",
      "Epoch 120/150\n",
      "330/330 [==============================] - 0s 437us/step - loss: 1.8954 - accuracy: 0.2960 - mse: 0.3972\n",
      "Epoch 121/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8923 - accuracy: 0.3004 - mse: 0.3976\n",
      "Epoch 122/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8953 - accuracy: 0.3017 - mse: 0.3984\n",
      "Epoch 123/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8855 - accuracy: 0.3070 - mse: 0.3985\n",
      "Epoch 124/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8896 - accuracy: 0.3003 - mse: 0.3979\n",
      "Epoch 125/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8827 - accuracy: 0.3027 - mse: 0.3995\n",
      "Epoch 126/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8879 - accuracy: 0.3066 - mse: 0.3979\n",
      "Epoch 127/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8820 - accuracy: 0.3022 - mse: 0.3987\n",
      "Epoch 128/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8828 - accuracy: 0.3045 - mse: 0.3989\n",
      "Epoch 129/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8788 - accuracy: 0.3052 - mse: 0.4011\n",
      "Epoch 130/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8778 - accuracy: 0.3071 - mse: 0.3993\n",
      "Epoch 131/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 1.8813 - accuracy: 0.3065 - mse: 0.4001\n",
      "Epoch 132/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8784 - accuracy: 0.3023 - mse: 0.4000\n",
      "Epoch 133/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8779 - accuracy: 0.3095 - mse: 0.4000\n",
      "Epoch 134/150\n",
      "330/330 [==============================] - 0s 421us/step - loss: 1.8789 - accuracy: 0.3112 - mse: 0.4004\n",
      "Epoch 135/150\n",
      "330/330 [==============================] - 0s 430us/step - loss: 1.8737 - accuracy: 0.3103 - mse: 0.4005\n",
      "Epoch 136/150\n",
      "330/330 [==============================] - 0s 421us/step - loss: 1.8710 - accuracy: 0.3104 - mse: 0.3999\n",
      "Epoch 137/150\n",
      "330/330 [==============================] - 0s 427us/step - loss: 1.8744 - accuracy: 0.3067 - mse: 0.4009\n",
      "Epoch 138/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8722 - accuracy: 0.3101 - mse: 0.4020\n",
      "Epoch 139/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 406us/step - loss: 1.8693 - accuracy: 0.3081 - mse: 0.4023\n",
      "Epoch 140/150\n",
      "330/330 [==============================] - 0s 421us/step - loss: 1.8691 - accuracy: 0.3085 - mse: 0.4035\n",
      "Epoch 141/150\n",
      "330/330 [==============================] - 0s 427us/step - loss: 1.8719 - accuracy: 0.3092 - mse: 0.4024\n",
      "Epoch 142/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 1.8703 - accuracy: 0.3037 - mse: 0.4054\n",
      "Epoch 143/150\n",
      "330/330 [==============================] - 0s 409us/step - loss: 1.8668 - accuracy: 0.3136 - mse: 0.4043\n",
      "Epoch 144/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8654 - accuracy: 0.3105 - mse: 0.4042\n",
      "Epoch 145/150\n",
      "330/330 [==============================] - 0s 412us/step - loss: 1.8681 - accuracy: 0.3076 - mse: 0.4059\n",
      "Epoch 146/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8621 - accuracy: 0.3142 - mse: 0.4058\n",
      "Epoch 147/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8626 - accuracy: 0.3085 - mse: 0.4070\n",
      "Epoch 148/150\n",
      "330/330 [==============================] - 0s 419us/step - loss: 1.8618 - accuracy: 0.3089 - mse: 0.4054\n",
      "Epoch 149/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8619 - accuracy: 0.3094 - mse: 0.4059\n",
      "Epoch 150/150\n",
      "330/330 [==============================] - 0s 415us/step - loss: 1.8607 - accuracy: 0.3069 - mse: 0.4047\n",
      "The neural network testing MSE is:   0.4099203449663348\n",
      "The neural network training MSE is:   0.4065026185747539\n",
      "INFO:tensorflow:Assets written to: NN_model.sav\\assets\n"
     ]
    }
   ],
   "source": [
    "# This code is to genereate the best testing model for the Neural Network for the application\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report as cr\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from scipy import stats\n",
    "\n",
    "# data preprocessing\n",
    "df = pd.read_csv('austinHousingData.csv')\n",
    "df_filtered = df[['livingAreaSqFt', 'numOfBathrooms', 'avgSchoolRating', 'numOfBedrooms', 'numOfHighSchools', 'MedianStudentsPerTeacher', 'latestPrice']]\n",
    "\n",
    "z = np.abs(stats.zscore(df_filtered))\n",
    "threshold = 2.5\n",
    "df_filtered_o = df_filtered[(z < threshold).all(axis=1)]\n",
    "\n",
    "df = df_filtered_o[['livingAreaSqFt', 'numOfBathrooms', 'avgSchoolRating', 'numOfBedrooms', 'numOfHighSchools', 'MedianStudentsPerTeacher', 'latestPrice']]\n",
    "df_filtered_o.to_csv(\"neuralNet_test.csv\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['buckets'] = None\n",
    "\n",
    "for idx,row in df.iterrows():\n",
    "  if (row['latestPrice']) < 50000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 0\n",
    "\n",
    "  elif (row['latestPrice']) < 100000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 1\n",
    "  \n",
    "  elif (row['latestPrice']) < 150000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 2\n",
    "\n",
    "  elif (row['latestPrice']) < 200000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 3\n",
    "\n",
    "  elif (row['latestPrice']) < 250000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 4\n",
    "\n",
    "  elif (row['latestPrice']) < 300000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 5\n",
    "\n",
    "  elif (row['latestPrice']) < 350000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 6\n",
    "\n",
    "  elif (row['latestPrice']) < 400000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 7\n",
    "\n",
    "  elif (row['latestPrice']) < 450000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 8\n",
    "\n",
    "  elif (row['latestPrice']) < 500000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 9\n",
    "\n",
    "  elif (row['latestPrice']) < 550000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 10\n",
    "\n",
    "  elif (row['latestPrice']) < 600000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 11\n",
    "\n",
    "  elif (row['latestPrice']) < 700000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 12\n",
    "\n",
    "  elif (row['latestPrice']) < 800000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 13\n",
    "\n",
    "  elif (row['latestPrice']) < 900000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 14\n",
    "\n",
    "  elif (row['latestPrice']) < 1000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 15\n",
    "\n",
    "  elif (row['latestPrice']) < 1500000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 16\n",
    "\n",
    "  elif (row['latestPrice']) < 2000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 17\n",
    "\n",
    "  elif (row['latestPrice']) < 3000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 18\n",
    "\n",
    "  elif (row['latestPrice']) < 4000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 19\n",
    "\n",
    "  elif (row['latestPrice']) < 5000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 20\n",
    "\n",
    "  elif (row['latestPrice']) < 6000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 21\n",
    "\n",
    "  elif (row['latestPrice']) < 7000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 22\n",
    "\n",
    "  elif (row['latestPrice']) < 8000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 23\n",
    "\n",
    "  elif (row['latestPrice']) < 9000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 24\n",
    "\n",
    "  elif (row['latestPrice']) >= 10000000:\n",
    "    \n",
    "    df.iloc[idx,-1] = 25\n",
    "\n",
    "df = df.drop(columns=['latestPrice'])\n",
    "\n",
    "X = df.iloc[:,:-1].to_numpy()\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "# using min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "joblib.dump(scaler, \"scalerNN.sav\")\n",
    "\n",
    "# One-hot encoder\n",
    "oHe = OneHotEncoder()\n",
    "y = df.iloc[:,-1].to_numpy().reshape([-1,1])\n",
    "y = oHe.fit_transform(y, y=None).toarray()\n",
    "\n",
    "# split the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, test_size = 0.2)\n",
    "\n",
    "# create and train the model\n",
    "NN_model = tf.keras.models.Sequential()\n",
    "NN_model.add(tf.keras.layers.Dense(100,activation='relu'))\n",
    "NN_model.add(tf.keras.layers.Dense(80,activation='relu'))\n",
    "NN_model.add(tf.keras.layers.Dense(50,activation='relu'))\n",
    "NN_model.add(tf.keras.layers.Dense(18,activation='sigmoid'))\n",
    "\n",
    "NN_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy','mse'])\n",
    "NN_model.optimizer.lr = 0.2\n",
    "NN_model.fit(X_train, y_train, epochs=150)\n",
    "\n",
    "y_pred_train = NN_model.predict(X_train)\n",
    "y_pred_test = NN_model.predict(X_test)\n",
    "\n",
    "mse_nn_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_nn_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"The neural network testing MSE is:  \", mse_nn_test)\n",
    "print(\"The neural network training MSE is:  \", mse_nn_train)\n",
    "\n",
    "tf.keras.models.save_model(NN_model, \"NN_model.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da90dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
